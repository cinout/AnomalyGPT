deepspeed==0.9.2
easydict==1.10
einops==0.6.1
ftfy==6.1.1
gradio==3.41.2
h5py==3.9.0
iopath==0.1.10
ipdb==0.13.13
kornia==0.7.0
matplotlib==3.7.2
mdtex2html==1.2.0
numpy==1.24.3
open3d_python==0.3.0.0
opencv_python==4.8.0.74
peft==0.3.0
Pillow==10.0.0
pytorchvideo==0.1.5
PyYAML==6.0.1
regex==2022.10.31
scikit_learn==1.3.0
termcolor==2.3.0
timm==0.6.7
# torch==1.13.1+cu117
# torchaudio==0.13.1
# torchvision==0.14.1+cu117
tqdm==4.64.1
transformers==4.29.1
sentencepiece

# ~/.conda/envs/anogpt/bin/pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117 --trusted-host download.pytorch.org



# [合并 LLAMA 和 Vicuna 权重] vicuna_ckpt (V0_7B) = LLaMA + delta weights of Vicuna provided by the original authors  
# [Vicuna 拥有正常的 LLM 能力]
# [存疑: Vicuna V0版本训练词表的维度和llama词表的维度不一致，V0版本词表是32001，而llama词表是32000，因此对输入embedding编码的参数维度不一致.]


# The image encoder is ImageBind (is text encoder also ImageBind???)

# [组合 Vicuna 权重和 PandaGPT 的权重] We use the pre-trained parameters from PandaGPT to initialize our AnomalyGPT. Our AnomalyGPT weights are also downloadable (I guess those are already trained??), put in the ./code/ckpt directory
# [PandaGPT 拥有正常的多模态对话能力, 合并 AnomalyGPT 的参数后模型拥有异常检测能力]

# --data_path: The data path for the json file pandagpt4_visual_instruction_data.json.
# --image_root_path: The root path for training images of PandaGPT.
# --imagebind_ckpt_path: The path of ImageBind checkpoint.
# --vicuna_ckpt_path: The directory that saves the pre-trained Vicuna checkpoints.

# [为啥微调mvtec数据的时候还需要同时训练 pandagpt4_visual_instruction_data.json 和images?] [ANS: pandagpt的数据是用来数据平衡，防止过拟合下游数据集和灾难性遗忘的，建议放上去一起训练。如果不用pandagpt的数据，需要修改一下 train_xxx.py 里面数据读取部分的代码]

# [可以用预训练模型AnomalyGPT weights，继续finetune模型，在 agent.py 文件中加载现有的 AnomalyGPT 权重，然后训练 https://github.com/CASIA-IVA-Lab/AnomalyGPT/issues/48]

# [label_mode: binary 和 logistic-intensity 的区别 在NSA文章里提及，用 gt>0.3 卡了一个模拟异常的阈值]
# [模拟的异常数据是用来训练 image decoder 和 prompt learner 的]
# [没有 mask 的是 pandagpt 原来的训练数据中的图像，这部分图像没有类别信息，所以统一使用 'object']
# [unsupervised 设置指的是不使用真实异常样本，我们使用了训练集中的真实正常样本和合成的模拟异常样本。]
# [Our current few-shot localizaiton result is based on the patch-level feature similarity between the query image and the normal image. Because there is no parameter training for the few-shot test samples, the localization result is not as accurate as the setting that has undergone unsupervised training.]

# [Our "few-shot inference" entails conducting inference on a particular class of objects that have never been seen during the training phase, with only few normal samples provided as references, so we do not need few-shot training. In the experiments of our paper, we unsupervisedly train our model on the Visa dataset, and employ this trained model directly for performing few-shot inference on the MVTec dataset. Conversely, models trained unsupervisedly on the MVTec dataset are employed for few-shot inference on the VisA dataset. So if you want to test the few-shot inference effect of k=1,2,4 on the MVTec dataset, you can directly use the unsupervised training parameters on VisA dataset, and then set the value of "k_shot" in ./code/test_mvtec.py to experiment.]

# [运行train_all_supervised_cn.sh: https://github.com/CASIA-IVA-Lab/AnomalyGPT/issues/4]
# [train_all_supervised_cn.sh 训练得到的模型只用于我们的在线 demo 展示，论文中提到的数据结果都是在无监督训练设置下得到的。]
# [test_mvtec.sh 默认是使用在 VisA 数据集中训练的参数进行 few-shot 测试，如果要测试 unsupervised 的性能，需要修改为在 mvtec 数据集中训练得到的参数，并将 FEW_SHOT 参数设置为 False.]

# [我们的 LLM 和 prompt learner 也会回传梯度到 Image Decoder 中，用文本信息指导 Image Decoder 更新参数]
# [是用了cross entropy文本的token对齐分类训练的prompt learner，用这个部分的损失去影响decoder部分]
# [Few-shot 的方法中计算异常定位的部分确实是一个多层特征的 patchCore，我们参考了 APRIL-GAN 的设计，性能提升应该和多层级特征、backbone 等都有关系。但是我们同时把这些输出特征和大语言模型对齐了，从而大语言模型能对输入图像内容进行更丰富的描述和精确判断。]
# 